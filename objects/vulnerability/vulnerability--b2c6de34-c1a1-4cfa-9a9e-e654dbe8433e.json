{
    "type": "bundle",
    "id": "bundle--c44468af-2baa-4f76-a401-6c4ed32ef97d",
    "objects": [
        {
            "type": "vulnerability",
            "spec_version": "2.1",
            "id": "vulnerability--b2c6de34-c1a1-4cfa-9a9e-e654dbe8433e",
            "created_by_ref": "identity--8ce3f695-d5a4-4dc8-9e93-a65af453a31a",
            "created": "2025-03-21T00:22:20.996829Z",
            "modified": "2025-03-21T00:22:20.996829Z",
            "name": "CVE-2024-12704",
            "description": "A vulnerability in the LangChainLLM class of the run-llama/llama_index repository, version v0.12.5, allows for a Denial of Service (DoS) attack. The stream_complete method executes the llm using a thread and retrieves the result via the get_response_gen method of the StreamingGeneratorCallbackHandler class. If the thread terminates abnormally before the _llm.predict is executed, there is no exception handling for this case, leading to an infinite loop in the get_response_gen function. This can be triggered by providing an input of an incorrect type, causing the thread to terminate and the process to continue running indefinitely.",
            "external_references": [
                {
                    "source_name": "cve",
                    "external_id": "CVE-2024-12704"
                }
            ]
        }
    ]
}